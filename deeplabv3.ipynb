{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba307710-2b9a-42ff-8473-84f8f762f3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 20 16:17:29 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:03:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    50W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    53W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM...  On   | 00000000:82:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    51W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM...  On   | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    54W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50689de4-d75b-45a4-92c3-46a8a3ca05ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e1e63-1f0f-4035-91bf-2c20cc46451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54187c65-8ac1-452c-a0ee-ab87b5d4a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75600af8-fdac-485c-ba96-384ca69d1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32843c56-bbcb-42e4-90eb-b7ae76a674f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39657f26-1967-4a3e-8e14-bfd7472956bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchvision.models.segmentation.deeplabv3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bfc214-8662-4072-908e-918e9a1b122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c12a6d-81be-4120-93a5-6c95ad3e92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Material:\n",
    " \n",
    "  def __init__(self, name, input_rgb_vals, output_val, confidence_threshold=0):\n",
    "    self.name = name\n",
    "    self.input_rgb_vals = input_rgb_vals\n",
    "    self.output_val = output_val\n",
    "    self.confidence_threshold = confidence_threshold\n",
    " \n",
    "#Creating a list of materials so we can iterate through it\n",
    "materials = [\n",
    "             Material(\"amf\", [50,50,50], 50, 0.55),\n",
    "             Material(\"background\", [255,255,255], 255, 0.4),\n",
    "             \n",
    "             ]\n",
    " \n",
    "num_materials =len(materials)\n",
    "#Various input/output directories\n",
    "training_image_directory = \"/pscratch/sd/d/drippner/Reslice Images/\"\n",
    "training_mask_directory = \"/pscratch/sd/d/drippner/Reslice Masks/\"\n",
    "#Fraction of total annotations you want to leave for validating the model.\n",
    "validation_fraction=0.2\n",
    "#Model Performance varies, make multiple models to have the best chance at success.\n",
    "num_models=5\n",
    "#Model Performance improves with increasing epochs, to a point.\n",
    "num_epochs=500\n",
    "\"\"\"Increasing batch size increase model training speed, but also eats up VRAM on the GPU. Find a balance between scale and batch size\n",
    "that best suits your needs\"\"\"\n",
    "batch_size=2\n",
    "#Decrease scale to decrease VRAM usage; if you run out of VRAM during traing, restart your runtime and down scale your images\n",
    "scale=1\n",
    "#Input model directory\n",
    "models_directory = \"/pscratch/sd/d/drippner/best_models/\"\n",
    "#Input the name you want to use for your group of models\n",
    "model_group='new amf p2 1p 100 epoch/'\n",
    "current_model_name = 'new amf p2 1p 100 epoch'\n",
    "\"\"\"Hold images/annotations in reserve to test your model performance. Use this metric to decide which model you want to use \n",
    "for your data analysis\"\"\"\n",
    "test_images = \"/pscratch/sd/d/drippner/Reslice Images/\"\n",
    "test_masks= \"/pscratch/sd/d/drippner/Reslice Masks/\"\n",
    "csv_directory = \"/pscratch/sd/d/drippner/amf test deeplabv3.csv\"\n",
    "#Input the directory of the data you want to segment here.\n",
    "# inference_directory= 'drive/MyDrive/dani/column_jpegs/Column1JPEG/'\n",
    "#inference_directory= 'drive/MyDrive/ALS Workflow/tiina new soil/'\n",
    "# inference_directory= 'drive/MyDrive/ALS Workflow/unf soil aggregate/rec20200215_203743_Devin_UNFsoilaggregate_convertedint/'\n",
    "inference_directory=\"/pscratch/sd/d/drippner/Reslice Images/\"\n",
    "\n",
    "#Input the 5 alpha-numeric characters proceding the file number of your images\n",
    "  #EX. Jmic3111_S0_GRID image_0.tif ----->mage_\n",
    "proceeding=\"1h1\"\n",
    "#Input the 4 or mor alpha-numeric characters following the file number\n",
    "  #EX. Jmic3111_S0_GRID image_0.tif ----->.tif\n",
    "following=\".png\"\n",
    "output_directory = \"/pscratch/sd/d/drippner/amf test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b51f980d-3164-4a16-9d08-39e6808e163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3232, 0.3232, 0.3232], dtype=torch.float64)\n",
      "tensor([0.2015, 0.2015, 0.2015], dtype=torch.float64)\n",
      "[0.32321666 0.32321666 0.32321666]\n",
      "tensor([-3.5774e-16, -3.5774e-16, -3.5774e-16], dtype=torch.float64)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#Code Box 4\n",
    "from os.path import splitext\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "from PIL import Image\n",
    "import random\n",
    "#import scipy.ndimage as ndi\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.ndimage import morphology\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    " \n",
    "class BasicDataset(Dataset):\n",
    "    def __init__(self, imgs_dir, masks_dir, scale=scale, transform=False):\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.scale = scale\n",
    "        self.transform=transform\n",
    "        self.t_list=A.Compose([A.HorizontalFlip(p=0.4),A.VerticalFlip(p=0.4), A.Rotate(limit=(-50, 50), p=0.4),])\n",
    "        self.means=[0,0,0]\n",
    "        self.stds=[1,1,1]\n",
    "\n",
    "        \n",
    "        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n",
    " \n",
    "        self.ids = [splitext(file)[0] for file in listdir(imgs_dir)\n",
    "                    if not file.startswith('.')]\n",
    "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    " \n",
    " \n",
    "    @classmethod\n",
    "    def mask_preprocess(cls, pil_img, scale):\n",
    "        w, h = pil_img.size\n",
    "        newW, newH = int(scale * w), int(scale * h)\n",
    "        assert newW > 0 and newH > 0, 'Scale is too small'\n",
    "        pil_img = pil_img.resize((newW, newH))\n",
    " \n",
    "        img_nd = np.array(pil_img)\n",
    " \n",
    "        if len(img_nd.shape) == 2:\n",
    "            img_nd = np.expand_dims(img_nd, axis=2)\n",
    " \n",
    "       \n",
    "        return img_nd\n",
    "    \n",
    " \n",
    "        \n",
    "    def img_preprocess(cls, pil_img, scale):\n",
    "        w, h = pil_img.size\n",
    "        newW, newH = int(scale * w), int(scale * h)\n",
    "        assert newW > 0 and newH > 0, 'Scale is too small'\n",
    "        pil_img = pil_img.resize((newW, newH))\n",
    " \n",
    "        img_nd = np.array(pil_img)\n",
    " \n",
    "        if len(img_nd.shape) == 2:\n",
    "            img_nd = np.expand_dims(img_nd, axis=2)\n",
    " \n",
    "       \n",
    " \n",
    "        return img_nd\n",
    " \n",
    "    def __getitem__(self, i):\n",
    "        idx = self.ids[i]\n",
    "        mask_file = glob(self.masks_dir + idx + '*')\n",
    "        img_file = glob(self.imgs_dir + idx + '*')\n",
    " \n",
    "        assert len(mask_file) == 1, \\\n",
    "            f'Either no mask or multiple masks found for the ID {idx}: {mask_file}'\n",
    "        assert len(img_file) == 1, \\\n",
    "            f'Either no image or multiple images found for the ID {idx}: {img_file}'\n",
    "        mask = Image.open(mask_file[0])\n",
    "        img = Image.open(img_file[0])\n",
    " \n",
    "  \n",
    "        \n",
    " \n",
    "        \n",
    "        #Reshapes from 1 channel to 3 channels in grayscale\n",
    "        img = self.img_preprocess(img, self.scale)\n",
    "        mask = self.mask_preprocess(mask, self.scale)\n",
    "        new_image=np.zeros((img.shape[0],img.shape[1],3))\n",
    "        new_image[:,:,0]=img[:,:,0]\n",
    "        new_image[:,:,1]=img[:,:,0]\n",
    "        new_image[:,:,2]=img[:,:,0]\n",
    "        \n",
    " \n",
    " \n",
    " \n",
    "        img=new_image\n",
    " \n",
    "        #New Code\n",
    "        masklist=[]\n",
    "            \n",
    "        for i, mat in enumerate(materials):\n",
    "        \n",
    "          indices = np.all(mask == mat.input_rgb_vals, axis=-1)\n",
    "          new_mask=np.zeros((img.shape[0],img.shape[1]))\n",
    "          new_mask[indices] = 1\n",
    "          masklist.append(new_mask)\n",
    " \n",
    "        mask=masklist\n",
    "  \n",
    " \n",
    "        \n",
    "        if img.max() > 1:\n",
    "            img = img / 255\n",
    " \n",
    "       \n",
    " \n",
    "        \n",
    "        if self.transform:\n",
    "            augmented=self.t_list(image=img, masks=mask)\n",
    "            img=augmented[\"image\"]\n",
    "            mask=augmented[\"masks\"]\n",
    "            \n",
    " \n",
    "        \n",
    " \n",
    "        img = img.transpose((2, 0, 1))\n",
    "        \n",
    "        mask=np.array(mask)\n",
    "        \n",
    "        \n",
    " \n",
    "        \n",
    " \n",
    "        img=torch.from_numpy(img)\n",
    "        mask=torch.from_numpy(mask)\n",
    "        \n",
    "        img=transforms.Normalize(mean=self.means, std=self.stds)(img)\n",
    "        return img, mask #img_file[0]\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, imgs_dir, scale=scale, transform=False):\n",
    "        self.imgs_dir = imgs_dir\n",
    "        # self.masks_dir = masks_dir\n",
    "        self.scale = scale\n",
    "        self.transform=transform\n",
    "        self.t_list=A.Compose([A.HorizontalFlip(p=0.4),A.VerticalFlip(p=0.4), A.Rotate(limit=(-50, 50), p=0.4),])\n",
    "        self.means=[0,0,0]\n",
    "        self.stds=[1,1,1]\n",
    "\n",
    "        \n",
    "        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n",
    " \n",
    "        self.ids = [splitext(file)[0] for file in listdir(imgs_dir)\n",
    "                    if not file.startswith('.')]\n",
    "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    " \n",
    " \n",
    "    @classmethod\n",
    "    def mask_preprocess(cls, pil_img, scale):\n",
    "        w, h = pil_img.size\n",
    "        newW, newH = int(scale * w), int(scale * h)\n",
    "        assert newW > 0 and newH > 0, 'Scale is too small'\n",
    "        pil_img = pil_img.resize((newW, newH))\n",
    " \n",
    "        img_nd = np.array(pil_img)\n",
    " \n",
    "        if len(img_nd.shape) == 2:\n",
    "            img_nd = np.expand_dims(img_nd, axis=2)\n",
    " \n",
    "       \n",
    "        return img_nd\n",
    "    \n",
    " \n",
    "        \n",
    "    def img_preprocess(cls, pil_img, scale):\n",
    "        w, h = pil_img.size\n",
    "        newW, newH = int(scale * w), int(scale * h)\n",
    "        assert newW > 0 and newH > 0, 'Scale is too small'\n",
    "        pil_img = pil_img.resize((newW, newH))\n",
    " \n",
    "        img_nd = np.array(pil_img)\n",
    " \n",
    "        if len(img_nd.shape) == 2:\n",
    "            img_nd = np.expand_dims(img_nd, axis=2)\n",
    " \n",
    "       \n",
    " \n",
    "        return img_nd\n",
    " \n",
    "    def __getitem__(self, i):\n",
    "        idx = self.ids[i]\n",
    "        # mask_file = glob(self.masks_dir + idx + '*')\n",
    "        img_file = glob(self.imgs_dir + idx + '*')\n",
    " \n",
    "        # assert len(mask_file) == 1, \\\n",
    "        #     f'Either no mask or multiple masks found for the ID {idx}: {mask_file}'\n",
    "        assert len(img_file) == 1, \\\n",
    "            f'Either no image or multiple images found for the ID {idx}: {img_file}'\n",
    "        # mask = Image.open(mask_file[0])\n",
    "        img = Image.open(img_file[0])\n",
    " \n",
    "  \n",
    "        \n",
    " \n",
    "        \n",
    "        #Reshapes from 1 channel to 3 channels in grayscale\n",
    "        img = self.img_preprocess(img, self.scale)\n",
    "        # mask = self.mask_preprocess(mask, self.scale)\n",
    "        new_image=np.zeros((img.shape[0],img.shape[1],3))\n",
    "        new_image[:,:,0]=img[:,:,0]\n",
    "        new_image[:,:,1]=img[:,:,0]\n",
    "        new_image[:,:,2]=img[:,:,0]\n",
    "        \n",
    " \n",
    " \n",
    " \n",
    "        img=new_image\n",
    " \n",
    "        #New Code\n",
    "        # masklist=[]\n",
    "            \n",
    "        # for i, mat in enumerate(materials):\n",
    "        \n",
    "        #   indices = np.all(mask == mat.input_rgb_vals, axis=-1)\n",
    "        #   new_mask=np.zeros((img.shape[0],img.shape[1]))\n",
    "        #   new_mask[indices] = 1\n",
    "        #   masklist.append(new_mask)\n",
    " \n",
    "        # mask=masklist\n",
    "  \n",
    " \n",
    "        \n",
    "        if img.max() > 1:\n",
    "            img = img / 255\n",
    " \n",
    "       \n",
    " \n",
    "        \n",
    "        if self.transform:\n",
    "            augmented=self.t_list(image=img)#, masks=mask)\n",
    "            img=augmented[\"image\"]\n",
    "            # mask=augmented[\"masks\"]\n",
    "            \n",
    " \n",
    "        \n",
    " \n",
    "        img = img.transpose((2, 0, 1))\n",
    "        \n",
    "        # mask=np.array(mask)\n",
    "        \n",
    "        \n",
    " \n",
    "        \n",
    " \n",
    "        img=torch.from_numpy(img)\n",
    "        # mask=torch.from_numpy(mask)\n",
    "        \n",
    "        img=transforms.Normalize(mean=self.means, std=self.stds)(img)\n",
    "        return img, img_file[0]\n",
    "\n",
    "        \n",
    "        \n",
    "dataset = BasicDataset(training_image_directory, training_mask_directory, scale=scale, transform=False)\n",
    " \n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!Set batch size here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "nimages = 0\n",
    "mean = 0. \n",
    "std = 0.\n",
    "for batch, _ in train_loader:\n",
    "    # Rearrange batch to be the shape of [B, C, W * H]\n",
    "    batch = batch.view(batch.size(0), batch.size(1), -1)\n",
    "    # Update total number of images\n",
    "    nimages += batch.size(0)\n",
    "    # Compute mean and std here\n",
    "    mean += batch.mean(2).sum(0) \n",
    "    std += batch.std(2).sum(0)\n",
    " \n",
    "# Final step\n",
    "mean /= nimages\n",
    "std /= nimages\n",
    " \n",
    "print(mean)\n",
    "print(std)\n",
    "\n",
    "m=mean.numpy()\n",
    "np.save(\"mean_array.npy\",m)\n",
    "\n",
    "s=std.numpy()\n",
    "np.save(\"std_array.npy\",s)\n",
    "\n",
    "print(m)\n",
    "# m=np.array2string(m)\n",
    "\n",
    "# print(m)\n",
    "\n",
    "# m.to_csv(means)\n",
    "\n",
    "# print(m.split(\"[]\"))\n",
    "\n",
    "# ''.join(filter(str.isdigit, m))\n",
    "\n",
    "dataset.means=mean\n",
    "dataset.stds=std \n",
    "\n",
    "nimages = 0\n",
    "means = 0.\n",
    "stds = 0.\n",
    "for batch, _ in train_loader:\n",
    "    # Rearrange batch to be the shape of [B, C, W * H]\n",
    "    batch = batch.view(batch.size(0), batch.size(1), -1)\n",
    "    # Update total number of images\n",
    "    nimages += batch.size(0)\n",
    "    # Compute mean and std here\n",
    "    means += batch.mean(2).sum(0) \n",
    "    stds += batch.std(2).sum(0)\n",
    " \n",
    "# Final step\n",
    "means /= nimages\n",
    "stds /= nimages\n",
    " \n",
    "print(means)\n",
    "print(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c10bf2-9487-4ece-aaa2-37a2d70a24fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7\n",
      "Epoch:  0\n",
      "Train loss: tensor(0.6011)\n",
      "Train loss: tensor(0.7375)\n",
      "Train loss: tensor(0.6027)\n",
      "Train loss: tensor(0.5616)\n",
      "Train loss: tensor(0.5446)\n",
      "Train loss: tensor(0.4950)\n",
      "Train loss: tensor(0.4493)\n",
      "Model is bad!, Current loss: tensor(nan) Best loss: 999\n",
      "\n",
      "\n",
      "11.7\n",
      "Epoch:  1\n",
      "Train loss: tensor(0.4408)\n",
      "Train loss: tensor(0.4092)\n",
      "Train loss: tensor(0.4085)\n",
      "Train loss: tensor(0.4018)\n",
      "Train loss: tensor(0.4086)\n",
      "Train loss: tensor(0.4147)\n",
      "Train loss: tensor(0.4015)\n",
      "Model is bad!, Current loss: tensor(nan) Best loss: 999\n",
      "\n",
      "\n",
      "11.7\n",
      "Epoch:  2\n",
      "Train loss: tensor(0.4042)\n",
      "Train loss: tensor(0.4058)\n",
      "Train loss: tensor(0.3920)\n",
      "Train loss: tensor(0.3953)\n",
      "Train loss: tensor(0.3973)\n",
      "Train loss: tensor(0.4074)\n",
      "Train loss: tensor(0.4039)\n",
      "Model is bad!, Current loss: tensor(nan) Best loss: 999\n",
      "\n",
      "\n",
      "11.7\n",
      "Epoch:  3\n",
      "Train loss: tensor(0.4061)\n",
      "Train loss: tensor(0.3967)\n",
      "Train loss: tensor(0.3817)\n",
      "Train loss: tensor(0.3766)\n",
      "Train loss: tensor(0.3993)\n",
      "Train loss: tensor(0.3949)\n",
      "Train loss: tensor(0.3991)\n",
      "Best Model Saved!, loss: tensor(263.4803)\n",
      "\n",
      "\n",
      "11.7\n",
      "Epoch:  4\n",
      "Train loss: tensor(0.4008)\n",
      "Train loss: tensor(0.3917)\n",
      "Train loss: tensor(0.3908)\n",
      "Train loss: tensor(0.3848)\n",
      "Train loss: tensor(0.3794)\n",
      "Train loss: tensor(0.3830)\n",
      "Train loss: tensor(0.3891)\n",
      "Best Model Saved!, loss: tensor(21.2095)\n",
      "\n",
      "\n",
      "11.7\n",
      "Epoch:  5\n",
      "Train loss: tensor(0.3830)\n",
      "Train loss: tensor(0.3770)\n",
      "Train loss: tensor(0.3905)\n",
      "Train loss: tensor(0.3863)\n",
      "Train loss: tensor(0.3921)\n",
      "Train loss: tensor(0.3834)\n",
      "Train loss: tensor(0.3903)\n",
      "Best Model Saved!, loss: tensor(0.9656)\n",
      "\n",
      "\n",
      "11.7\n",
      "Epoch:  6\n",
      "Train loss: tensor(0.3906)\n",
      "Train loss: tensor(0.3960)\n",
      "Train loss: tensor(0.4028)\n",
      "Train loss: tensor(0.3993)\n",
      "Train loss: tensor(0.3770)\n",
      "Train loss: tensor(0.3804)\n",
      "Train loss: tensor(0.3857)\n",
      "Model is bad!, Current loss: tensor(1.0687) Best loss: tensor(0.9656)\n",
      "\n",
      "\n",
      "11.7\n",
      "Epoch:  7\n",
      "Train loss: tensor(0.3867)\n",
      "Train loss: tensor(0.3832)\n",
      "Train loss: tensor(0.3968)\n",
      "Train loss: tensor(0.3791)\n",
      "Train loss: tensor(0.3930)\n",
      "Train loss: tensor(0.3942)\n",
      "Train loss: tensor(0.3842)\n",
      "Model is bad!, Current loss: tensor(1.1469) Best loss: tensor(0.9656)\n",
      "\n",
      "\n",
      "11.7\n",
      "Epoch:  8\n",
      "Train loss: tensor(0.3728)\n",
      "Train loss: tensor(0.3858)\n",
      "Train loss: tensor(0.3951)\n",
      "Train loss: tensor(0.3993)\n",
      "Train loss: tensor(0.3912)\n",
      "Train loss: tensor(0.4080)\n",
      "Train loss: tensor(0.3834)\n",
      "Model is bad!, Current loss: tensor(1.1523) Best loss: tensor(0.9656)\n",
      "\n",
      "\n",
      "11.7\n",
      "Epoch:  9\n",
      "Train loss: tensor(0.3821)\n",
      "Train loss: tensor(0.3909)\n",
      "Train loss: tensor(0.3972)\n",
      "Train loss: tensor(0.3933)\n",
      "Train loss: tensor(0.4031)\n",
      "Train loss: tensor(0.3823)\n",
      "Train loss: tensor(0.4046)\n",
      "Model is bad!, Current loss: tensor(1.1471) Best loss: tensor(0.9656)\n",
      "\n",
      "\n",
      "11.7\n",
      "Epoch:  10\n",
      "Train loss: tensor(0.4024)\n",
      "Train loss: tensor(0.3892)\n",
      "Train loss: tensor(0.3990)\n",
      "Train loss: tensor(0.4039)\n",
      "Train loss: tensor(0.4085)\n",
      "Train loss: tensor(0.4074)\n",
      "Train loss: tensor(0.4096)\n",
      "Model is bad!, Current loss: tensor(1.0122) Best loss: tensor(0.9656)\n",
      "\n",
      "\n",
      "11.7\n",
      "Epoch:  11\n",
      "Train loss: tensor(0.4136)\n",
      "Train loss: tensor(0.3965)\n",
      "Train loss: tensor(0.4123)\n",
      "Train loss: tensor(0.3907)\n",
      "Train loss: tensor(0.3841)\n",
      "Train loss: tensor(0.3930)\n",
      "Train loss: tensor(0.3973)\n",
      "Model is bad!, Current loss: tensor(1.0454) Best loss: tensor(0.9656)\n",
      "\n",
      "\n",
      "11.7\n",
      "Epoch:  12\n",
      "Train loss: tensor(0.3968)\n",
      "Train loss: tensor(0.3964)\n",
      "Train loss: tensor(0.4010)\n",
      "Train loss: tensor(0.3818)\n",
      "Train loss: tensor(0.3701)\n",
      "Train loss: tensor(0.4091)\n",
      "Train loss: tensor(0.3972)\n",
      "Best Model Saved!, loss: tensor(0.8212)\n",
      "\n",
      "\n",
      "11.7\n",
      "Epoch:  13\n",
      "Train loss: tensor(0.4126)\n",
      "Train loss: tensor(0.4084)\n",
      "Train loss: tensor(0.3961)\n",
      "Train loss: tensor(0.4003)\n",
      "Train loss: tensor(0.3894)\n",
      "Train loss: tensor(0.3999)\n",
      "Train loss: tensor(0.3871)\n",
      "Model is bad!, Current loss: tensor(0.8419) Best loss: tensor(0.8212)\n",
      "\n",
      "\n",
      "11.7\n",
      "Epoch:  14\n",
      "Train loss: tensor(0.3865)\n",
      "Train loss: tensor(0.4095)\n",
      "Train loss: tensor(0.3979)\n",
      "Train loss: tensor(0.4023)\n",
      "Train loss: tensor(0.3971)\n",
      "Train loss: tensor(0.3815)\n",
      "Train loss: tensor(0.3995)\n",
      "Best Model Saved!, loss: tensor(0.6782)\n",
      "\n",
      "\n",
      "11.7\n",
      "Epoch:  15\n"
     ]
    }
   ],
   "source": [
    "#For loop for FCN model training Cell Code Box 5\n",
    "#!cd \"drive/My Drive/Colab Notebooks\"\n",
    "# Semantic Segmentation and Data Extraction in Pytorch Using FCN by Pranav Raja\n",
    " \n",
    "import torchvision\n",
    "# from torchvision.models.segmentation.fcn import FCNHead\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "# from torch._six import container_abcs, string_classes, int_classes\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import psutil\n",
    "import gc\n",
    "import random\n",
    " \n",
    "dir_checkpoint = models_directory\n",
    " \n",
    " \n",
    "model_group=model_group\n",
    "num_models=num_models\n",
    "os.mkdir(dir_checkpoint+model_group)\n",
    "seed=0\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "for i in range(num_models):\n",
    "  #!!!!!!! Here we pull in a pretrained FCN on torch and we replace the output layer since we have six classes rather than 21!!!!!!!!\n",
    "  num_classes=num_materials\n",
    "  model=torchvision.models.segmentation.deeplabv3_resnet101(pretrained=False, progress=True)\n",
    "  model.classifier=DeepLabHead(2048, num_classes)\n",
    "  \n",
    "  def trainval_split(dataset, val_fraction=0.5):\n",
    " \n",
    "    validation_size = int(len(dataset) * val_fraction)\n",
    "    train_size = len(dataset) - validation_size\n",
    "    train, val = torch.utils.data.random_split(dataset, [train_size, validation_size], generator=torch.Generator().manual_seed(i))\n",
    " \n",
    "    return train, val\n",
    " \n",
    " \n",
    " \n",
    "  dataset= BasicDataset(training_image_directory, training_mask_directory, scale=scale, transform=True)\n",
    "  dataset_train, dataset_val=trainval_split(dataset, val_fraction=validation_fraction)\n",
    "\n",
    "  train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)#, collate_fn=pad_collate)\n",
    "  val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)#, collate_fn=pad_collate)\n",
    " \n",
    " \n",
    "  #%%\n",
    " \n",
    "  # this is the train code \n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  model= nn.DataParallel(model)\n",
    "  model.to(device)\n",
    "  #!!!!!!!!!!!!!!!!!!!!!!!!!!!! Input epochs here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "  num_epochs=num_epochs\n",
    "  # read up on optimizers but Adam should work for now, if you get good results with Adam then you can try SGD (it's harder to tune but usually converges better)\n",
    "  optimizer=torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    " \n",
    "  #just initializing a value called best_loss\n",
    "  best_loss=999\n",
    " \n",
    "  # choose a loss function\n",
    "\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "  #this is the train loop\n",
    "  for epoch in range(num_epochs):\n",
    "      print(psutil.virtual_memory().percent)\n",
    "      print('Epoch: ', str(epoch))\n",
    "    #add back if doing fractional training\n",
    "      train_loader.dataset.dataset.transform=True\n",
    "      model.train()\n",
    "      for images, masks in train_loader:\n",
    " \n",
    "          images = images.to(device=device, dtype=torch.float16) #11111111\n",
    "          masks = masks.to(device=device, dtype=torch.float16)\n",
    "          \n",
    "          with torch.cuda.amp.autocast(): \n",
    "              #forward pass\n",
    "              preds=model(images)['out'].cuda()\n",
    "\n",
    "              #compute loss\n",
    "              loss=criterion(preds, masks)\n",
    "        \n",
    "          #reset the optimizer gradients to 0\n",
    "          optimizer.zero_grad()\n",
    " \n",
    "          #backward pass (compute gradients)\n",
    "          loss.backward()\n",
    " \n",
    "          #use the computed gradients to update model weights\n",
    "          optimizer.step()\n",
    " \n",
    "          print('Train loss: '+str(loss.to('cpu').detach()))\n",
    "      # model.eval()\n",
    "      #add back if doing fractional training\n",
    "      val_loader.dataset.dataset.transform=False\n",
    "      current_loss=0\n",
    "      \n",
    "      #test on val set and save the best checkpoint\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device=device, dtype=torch.float16)\n",
    "            masks = masks.to(device=device, dtype=torch.float16)\n",
    "\n",
    "            \n",
    "            with torch.cuda.amp.autocast(): ##################################################################################\n",
    "                preds=model(images)['out'].cuda()\n",
    "                # print(preds)\n",
    "                # print(masks)\n",
    "                loss=criterion(preds, masks)\n",
    "            #print('hi')\n",
    "            current_loss+=loss.to('cpu').detach()\n",
    "            del images, masks, preds, loss\n",
    "  #!!!!!!!!!!!!!!!!!!!!!!!!!!!Re-name model here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!        \n",
    "      if best_loss>current_loss:\n",
    "          best_loss=current_loss\n",
    "          print('Best Model Saved!, loss: '+ str(best_loss))\n",
    "          torch.save(model.state_dict(), dir_checkpoint+model_group + current_model_name+str(i+1)+\".pth\")\n",
    "      else:\n",
    "          print('Model is bad!, Current loss: '+ str(current_loss) + ' Best loss: '+str(best_loss))\n",
    "      print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff7b8199-758b-415c-81e2-06a3c9a915b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amf p2 1p 100 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/1984061536.py:96: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  modeldata=modeldata.append([df], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>model number</th>\n",
       "      <th>model name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.041666668</td>\n",
       "      <td>0.9997401</td>\n",
       "      <td>0.041666668</td>\n",
       "      <td>1</td>\n",
       "      <td>amf p2 1p 100 epoch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>background</td>\n",
       "      <td>0.9581894</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9581894</td>\n",
       "      <td>0.95839864</td>\n",
       "      <td>1</td>\n",
       "      <td>amf p2 1p 100 epoch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name  precision       recall   accuracy           f1 model number  \\\n",
       "0         amf        1.0  0.041666668  0.9997401  0.041666668            1   \n",
       "1  background  0.9581894          1.0  0.9581894   0.95839864            1   \n",
       "\n",
       "            model name  \n",
       "0  amf p2 1p 100 epoch  \n",
       "1  amf p2 1p 100 epoch  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code Box 6\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision.models.segmentation.fcn import FCNHead\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "#from statistics import mean\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# interim_list=[]\n",
    "modeldata = pd.DataFrame(columns=[\"name\", \"precision\", \"recall\", \"accuracy\", \"f1\"])\n",
    "\n",
    " \n",
    "for s in range(num_models):\n",
    "\n",
    "  model=torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True, progress=True)\n",
    "  model.classifier=DeepLabHead(2048, num_materials)\n",
    " \n",
    "  # model=torchvision.models.segmentation.fcn_resnet101(pretrained=False)\n",
    "  #!!!!!!!!!!!!!!!!!Specify Layer # here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "  # model.classifier=FCNHead(2048, num_materials)\n",
    "  device = torch.device('cuda')\n",
    " \n",
    "  outputs=[]\n",
    "  model.to(device)\n",
    " \n",
    "  #!!!!!!!!!!!!!!!!!!!!!Select Correct Model from the best models directory!!!!!!!!!!!!!!!!!!!!!!!!!1\n",
    " \n",
    "  model.load_state_dict(torch.load(models_directory+model_group + current_model_name+str(s+1)+\".pth\"), strict=False)\n",
    " \n",
    " \n",
    "  model.eval()\n",
    " \n",
    "  dataset_val = BasicDataset(test_images, test_masks, scale=scale, transform=False)\n",
    "  val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)#, collate_fn=pad_collate)\n",
    " \n",
    "  prop_list = []\n",
    "  for mat in materials:\n",
    "    prop_list.append([[],[],[],[]])\n",
    " \n",
    "  for images, target in val_loader:\n",
    "    images = images.to(device=device, dtype=torch.float32)\n",
    "    target = target.to(device=device, dtype=torch.float32)\n",
    " \n",
    "    with torch.no_grad():\n",
    "      pred=model(images)['out'].cuda()\n",
    "      pred=nn.Sigmoid()(pred)\n",
    "    \n",
    "    for i, mat in enumerate(materials):\n",
    "      material_target=target[:,i,:,:]\n",
    "      material_pred = pred[:, i, :, :]\n",
    "      material_pred[material_pred >=mat.confidence_threshold] = 1\n",
    "      material_pred[material_pred <=mat.confidence_threshold] = 0\n",
    "      pred[:, i, :, :]=material_pred\n",
    " \n",
    "      material_tp=torch.sum(material_target*material_pred, (1,2))\n",
    "      material_fp=torch.sum((1-material_target)*material_pred, (1,2))\n",
    "      material_fn=torch.sum(material_target*(1-material_pred), (1,2))\n",
    "      material_tn=torch.sum((1-material_target)*(1-material_pred), (1,2))\n",
    " \n",
    "     \n",
    " \n",
    "      material_precision=torch.mean((material_tp+0.000000001)/(material_tp+0.000000001+material_fp))\n",
    "      material_recall=torch.mean((material_tp+0.000000001)/(material_tp+0.000000001+material_fn))\n",
    "      material_accuracy=torch.mean((material_tp+material_tn)/(material_tp+material_tn+material_fp+material_fn))\n",
    "      material_f1=torch.mean((material_tp+0.000000001)/((material_tp+0.000000001)+0.5*(material_fp+material_fn)))\n",
    " \n",
    "    \n",
    "      prop_list[i][0].append(material_precision.cpu().detach().numpy())\n",
    "      prop_list[i][1].append(material_recall.cpu().detach().numpy())\n",
    "      prop_list[i][2].append(material_accuracy.cpu().detach().numpy())\n",
    "      prop_list[i][3].append(material_f1.cpu().detach().numpy())\n",
    "      \n",
    " \n",
    "          \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "  model_name=current_model_name\n",
    "  model_number=(str(s+1))\n",
    "  print(model_name)\n",
    " \n",
    "  #printing with pandas\n",
    "  properties = {\"name\" : [mat.name for mat in materials],\n",
    "                \"precision\" : [str(np.mean(prop_list[i][0])) for i in range(num_materials)],\n",
    "                \"recall\" : [str(np.mean(prop_list[i][1])) for i in range(num_materials)],\n",
    "                \"accuracy\" : [str(np.mean(prop_list[i][2])) for i in range(num_materials)],\n",
    "                \"f1\" : [str(np.mean(prop_list[i][3])) for i in range(num_materials)]}\n",
    "  df = pd.DataFrame(properties, columns = [\"name\", \"precision\", \"recall\", \"accuracy\", \"f1\"])\n",
    "  df=pd.concat([df, pd.DataFrame(columns=[\"model number\",\"model name\"])])\n",
    "  df[[\"model number\",\"model name\"]]=[model_number, model_name]\n",
    "  # display(df)\n",
    "  \n",
    "  modeldata=modeldata.append([df], ignore_index=True)\n",
    "\n",
    "\n",
    "display(modeldata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a5fe292-7074-4c93-a14a-ba45089564a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>model number</th>\n",
       "      <th>model name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amf</td>\n",
       "      <td>0.44447887</td>\n",
       "      <td>0.3880566</td>\n",
       "      <td>0.7211492</td>\n",
       "      <td>0.11118</td>\n",
       "      <td>1</td>\n",
       "      <td>dl 5 materials soil bce p2 1p 100 epoch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>background</td>\n",
       "      <td>0.9440948</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9440948</td>\n",
       "      <td>0.9443667</td>\n",
       "      <td>1</td>\n",
       "      <td>dl 5 materials soil bce p2 1p 100 epoch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name   precision     recall   accuracy         f1 model number  \\\n",
       "0         amf  0.44447887  0.3880566  0.7211492    0.11118            1   \n",
       "1  background   0.9440948        1.0  0.9440948  0.9443667            1   \n",
       "\n",
       "                                model name  \n",
       "0  dl 5 materials soil bce p2 1p 100 epoch  \n",
       "1  dl 5 materials soil bce p2 1p 100 epoch  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code Box 7\n",
    "display(modeldata)\n",
    "modeldata.to_csv(csv_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2a97519-9416-414f-9860-9bc61967def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Box 8\n",
    "\"\"\"Input model number here\"\"\"\n",
    "model_number='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c446298b-b213-422e-8015-c29473073504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepLabV3(\n",
       "  (backbone): IntermediateLayerGetter(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (8): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (9): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (10): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (11): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (12): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (13): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (14): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (15): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (16): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (17): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (18): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (19): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (20): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (21): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (22): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): DeepLabHead(\n",
       "    (0): ASPP(\n",
       "      (convs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (3): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (4): ASPPPooling(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (project): Sequential(\n",
       "        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (aux_classifier): FCNHead(\n",
       "    (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code Box 9\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from torchvision.models.segmentation.fcn import FCNHead\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    " \n",
    " \n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "model=torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True, progress=True)\n",
    "model.classifier=DeepLabHead(2048, num_materials)\n",
    " \n",
    "device = torch.device('cuda')\n",
    " \n",
    "outputs=[]\n",
    "model.to(device)\n",
    " \n",
    "#!!!!!!!!!!!!!!!!!!!!!Select Correct Model from the best models directory!!!!!!!!!!!!!!!!!!!!!!!!!1\n",
    "model.load_state_dict(torch.load(models_directory+model_group + current_model_name+model_number+'.pth'), strict=False)\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ff4f837-2cdd-404f-96cf-340e7d497121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1920, 2000])\n",
      "/pscratch/sd/d/drippner/Reslice Images/Reslice of T4PmR2d1h11922.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//amf/1922_amf_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//background/1922_background_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[False, False, 1, False, False, 1]\n",
      "None\n",
      "torch.Size([1, 3, 1920, 2000])\n",
      "/pscratch/sd/d/drippner/Reslice Images/Reslice of T4PmR2d1h10282.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//amf/0282_amf_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//background/0282_background_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[False, False, 1, False, False, 1]\n",
      "None\n",
      "torch.Size([1, 3, 1920, 2000])\n",
      "/pscratch/sd/d/drippner/Reslice Images/Reslice of T4PmR2d1h10782.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//amf/0782_amf_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//background/0782_background_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[False, False, 1, False, False, 1]\n",
      "None\n",
      "torch.Size([1, 3, 1920, 2000])\n",
      "/pscratch/sd/d/drippner/Reslice Images/Reslice of T4PmR2d1h10484.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//amf/0484_amf_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//background/0484_background_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[False, False, 1, False, False, 1]\n",
      "None\n",
      "torch.Size([1, 3, 1920, 2000])\n",
      "/pscratch/sd/d/drippner/Reslice Images/Reslice of T4PmR2d1h10144.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//amf/0144_amf_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//background/0144_background_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[False, False, 1, False, False, 1]\n",
      "None\n",
      "torch.Size([1, 3, 1920, 2000])\n",
      "/pscratch/sd/d/drippner/Reslice Images/Reslice of T4PmR2d1h10382.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//amf/0382_amf_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//background/0382_background_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[False, False, 1, False, False, 1]\n",
      "None\n",
      "torch.Size([1, 3, 1920, 2000])\n",
      "/pscratch/sd/d/drippner/Reslice Images/Reslice of T4PmR2d1h11000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//amf/1000_amf_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//background/1000_background_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[False, False, 1, False, False, 1]\n",
      "None\n",
      "torch.Size([1, 3, 1920, 2000])\n",
      "/pscratch/sd/d/drippner/Reslice Images/Reslice of T4PmR2d1h10100.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//amf/0100_amf_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//background/0100_background_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[False, False, 1, False, False, 1]\n",
      "None\n",
      "torch.Size([1, 3, 1920, 2000])\n",
      "/pscratch/sd/d/drippner/Reslice Images/Reslice of T4PmR2d1h11892.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//amf/1892_amf_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//background/1892_background_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[False, False, 1, False, False, 1]\n",
      "None\n",
      "torch.Size([1, 3, 1920, 2000])\n",
      "/pscratch/sd/d/drippner/Reslice Images/Reslice of T4PmR2d1h10658.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37041/10124390.py:133: UserWarning: /pscratch/sd/d/drippner/amf test//amf/0658_amf_mask.png is a low contrast image\n",
      "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37041/10124390.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_dir_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfollowing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproceeding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_mask.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_dir_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfollowing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproceeding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_mask.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mnp_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/pm-2022q3/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, plugin, check_contrast, **plugin_args)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s is a boolean image: setting True to 1 and False to 0'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imsave'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/pm-2022q3/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m                                (plugin, kind))\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/pm-2022q3/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages/imageio/v2.py\u001b[0m in \u001b[0;36mimwrite\u001b[0;34m(uri, im, format, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mimopen_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"legacy_mode\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mimopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mimopen_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/pm-2022q3/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages/imageio/core/legacy_plugin_wrapper.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, ndimage, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_mode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m\"iv\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/pm-2022q3/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages/imageio/core/format.py\u001b[0m in \u001b[0;36mappend_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;31m# Call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mset_meta_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/pm-2022q3/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages/imageio/plugins/pillow_legacy.py\u001b[0m in \u001b[0;36m_append_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m                 \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_as_uint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbitdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0mPillowFormat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/pm-2022q3/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages/imageio/plugins/pillow_legacy.py\u001b[0m in \u001b[0;36m_append_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"bits\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Make it a P image, so bits arg is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugin_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m             \u001b[0msave_pillow_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/pm-2022q3/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2320\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2321\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2322\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mopen_fp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/pm-2022q3/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0m_write_multiple_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m         \u001b[0mImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_idat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/pm-2022q3/sw/python/3.9-anaconda-2021.11/lib/python3.9/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    516\u001b[0m                     \u001b[0;31m# compress to Python file-compatible object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                         \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m                         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import scipy as scipy\n",
    "\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from skimage import (\n",
    "    color, feature, filters, measure, morphology, segmentation, util)\n",
    "\n",
    "from glob import glob\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from scipy.ndimage import morphology\n",
    "sys.path.append(os.path.join(sys.path[0]))  # To find local version of the library\n",
    "from skimage.filters import sobel\n",
    "from scipy import ndimage\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch\n",
    "from skimage.color import rgb2gray, label2rgb\n",
    "from skimage.segmentation import slic, join_segmentations, watershed\n",
    "from skimage import morphology\n",
    "from skimage.morphology import selem\n",
    "\n",
    "\n",
    "from skimage import io,exposure, feature, filters, io, measure, morphology, restoration, segmentation, transform, util, data, color\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "import os\n",
    "import numpy as np\n",
    "input_path = inference_directory\n",
    "filenames = os.listdir(input_path)\n",
    "\n",
    "output_path=output_directory\n",
    "\n",
    "dataset_val = InferenceDataset(input_path, 1, transform=False)\n",
    "\n",
    "# dataset_val.means=means\n",
    "# dataset_val.stds=stds\n",
    "\n",
    "val_loader = DataLoader(dataset_val, batch_size=1,shuffle=False, num_workers=0, pin_memory=True)#, collate_fn=pad_collate)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "whole_leaf_tables=[]\n",
    "file_name=[]\n",
    "color=[]\n",
    "value_counts=[]\n",
    "\n",
    "for images, filename in val_loader:\n",
    "    filename=filename[0]\n",
    "    new_dir_name = output_path\n",
    "    if not os.path.exists(new_dir_name):\n",
    "      os.makedirs(new_dir_name)\n",
    "    \n",
    "    for mat in materials:\n",
    "      new_dir_name_mat= new_dir_name + mat.name\n",
    "      if not os.path.exists(new_dir_name_mat):\n",
    "        os.makedirs(new_dir_name_mat)\n",
    "    print(images.shape)\n",
    "    print(filename)\n",
    "    images = images.to(device=device, dtype=torch.float32)\n",
    "    # masks = masks.to(device=device, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "      preds=model(images)['out'].cuda()\n",
    "      preds=nn.Sigmoid()(preds)\n",
    "      mat_mask1=preds.cpu().detach().numpy()[0,0,:,:]\n",
    "      mat_mask1[mat_mask1 >= mat.confidence_threshold] = mat.output_val\n",
    "      mat_mask1[mat_mask1 < mat.confidence_threshold] = 0  \n",
    "      list_of_mat_tables = []\n",
    "      for i, mat in enumerate(materials):\n",
    "\n",
    "        mat_mask = preds.cpu().detach().numpy()[0,i,:,:]\n",
    "        # plt.imshow(mat_mask)\n",
    "        # plt.show()\n",
    "\n",
    "        mat_mask[mat_mask >= mat.confidence_threshold] = mat.output_val\n",
    "        mat_mask[mat_mask < mat.confidence_threshold] = 0\n",
    "        # plt.imshow(mat_mask)\n",
    "        # plt.show()\n",
    "\n",
    "        # distance = ndi.distance_transform_edt(mat_mask)\n",
    "\n",
    "        # local_max_coords = feature.peak_local_max(distance, min_distance=30)\n",
    "        # local_max_mask = np.zeros(distance.shape, dtype=bool)\n",
    "        # local_max_mask[tuple(local_max_coords.T)] = True\n",
    "        # markers = measure.label(local_max_mask)\n",
    "\n",
    "        # seg1 = segmentation.watershed(-distance, markers, mask=mat_mask, compactness=.5)\n",
    "        # seg1=np.array(seg1, dtype=float)\n",
    "        \n",
    "        # sobel = scipy.ndimage.sobel(mat_mask)\n",
    "        # # io.imshow(sobel)\n",
    "        # markers = np.zeros_like(sobel)\n",
    "        # foreground, background = 1, 2\n",
    "        # markers[sobel <= 0] = background\n",
    "        # markers[sobel >=1] = foreground\n",
    "        \n",
    "\n",
    "        # ws = watershed(sobel,mask=mat_mask, compactness=1)\n",
    "        \n",
    "        # seg1 = label(ws == foreground)\n",
    "        # selem=morphology.selem.disk(8)\n",
    "        # mat_mask=morphology.binary_erosion(mat_mask, selem=selem)\n",
    "\n",
    "\n",
    "        img=images.cpu().detach().numpy()[0,:,:,:]\n",
    "        img=np.transpose(img,(1,2,0))\n",
    "        img=rgb2gray(img)\n",
    "        img2=label(mat_mask, background=0)\n",
    "        img3=label2rgb(img2, img, alpha=0.3, bg_label=0)   \n",
    "   \n",
    "        # io.imsave(output_path+filename.split('.png')[0]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
    "        # io.imsave(new_dir_name+\"/\"+ mat.name + \"/\"+filename.split(\".\")[0].split(\"/\")[0]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
    "        # io.imsave(new_dir_name+\"/\"+filename.split(\".\")[0].split('/')[-1]+\"_mask.png\", img3)\n",
    "        # io.imsave(new_dir_name+'/'+filename.split(following)[0]+'.png', img3) \n",
    "\n",
    "        io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0].split(proceeding)[-1]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
    "        io.imsave(new_dir_name+'/'+filename.split(following)[0].split(proceeding)[-1]+'_'+\"_mask.png\", img3) \n",
    "       \n",
    "        np_mat = np.array(mat_mask)\n",
    "        \n",
    "        \n",
    "        label_mat = label(np_mat, background = 0)\n",
    "        \n",
    "        if np.sum(label_mat)<1:\n",
    "          label_mat=np.ones_like(label_mat)\n",
    "        elif np.sum(label_mat)>=1:\n",
    "          label_mat=label_mat\n",
    "       \n",
    "        \n",
    "        mat_table=measure.regionprops_table(label_mat, img, properties=['label','area','perimeter'])\n",
    "        print(len(mat_table['label']))\n",
    "        count=(len(mat_table['label']))\n",
    "        mat_table=pd.DataFrame(mat_table)\n",
    "        if mat_table['area'].sum()==np.ones_like(label_mat).sum():\n",
    "          mat_table_a=mat_table['area'].sum()==0\n",
    "        \n",
    "        elif mat_table['area'].sum()<np.ones_like(label_mat).sum():\n",
    "          mat_table_a=mat_table['area'].sum()\n",
    "\n",
    "        if mat_table['area'].sum()==np.ones_like(label_mat).sum():\n",
    "          mat_table_p=mat_table['perimeter'].sum()==0\n",
    "      \n",
    "        elif mat_table['area'].sum()<np.ones_like(label_mat).sum():\n",
    "          mat_table_p=mat_table['perimeter'].sum()\n",
    "\n",
    "\n",
    "\n",
    "        mat_table_c=count\n",
    "        # mat_table_p=mat_table['perimeter'].sum()\n",
    "\n",
    "        list_of_mat_tables.append(mat_table_a)\n",
    "        list_of_mat_tables.append(mat_table_p)\n",
    "        list_of_mat_tables.append(mat_table_c)\n",
    "        \n",
    "\n",
    "\n",
    "      # io.imshow(mat_mask[:,:])\n",
    "      print(list_of_mat_tables)\n",
    "      list_of_mat_tables = np.array(list_of_mat_tables, dtype=int)\n",
    "      whole_leaf = value_counts.append(list_of_mat_tables)\n",
    "      print(whole_leaf)\n",
    "\n",
    "      name = file_name.append([filename])\n",
    "      \n",
    "  \n",
    "counts=(np.array(value_counts))\n",
    "print(counts)\n",
    "names=(np.array(file_name))\n",
    "print(names)\n",
    "\n",
    "whole_leaf=np.concatenate((names, counts), axis=1)\n",
    "\n",
    "whole_leaf_table=(pd.DataFrame(whole_leaf))\n",
    "print(whole_leaf_table)\n",
    "table_columns = [[f\"{mat.name} area(pix)\", f\"{mat.name} perimeter (pix)\", f\"{mat.name} counts\"] for mat in materials]\n",
    "print(table_columns)\n",
    "table_columns = [element for sublist in table_columns for element in sublist] #flattening the list\n",
    "print(table_columns)\n",
    "table_columns = [\"file_name\"] + table_columns\n",
    "print(table_columns)\n",
    "whole_leaf_table.columns = table_columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_row',None)\n",
    "# print(d)\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!Can change table output name here!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "whole_leaf_table.to_csv(new_dir_name+'/'+'material area, perimeter, and counts.csv')\n",
    "#print (counts)\n",
    "print('----end-----')\n",
    "      \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a554b44f-b0b8-46b6-a623-caaaff089106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
